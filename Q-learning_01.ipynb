{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rewards\n",
    "RUN_OUT_GASOLINE = -100\n",
    "WHEAT_HARVESTED = 0\n",
    "EMPTY_CELL = -1\n",
    "OBSTACLE_ENCOUNTERED = -100\n",
    "GASOLINE_SPENT = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FarmEnv(gym.Env):\n",
    "    def __init__(self, grid_size, obstacles, combine):\n",
    "        super(FarmEnv, self).__init__()\n",
    "        self.grid_size = grid_size\n",
    "        self.action_space = spaces.Discrete(4)  # Actions: up, down, left, right, harvest\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(grid_size[0], grid_size[1], 3), dtype=np.float32)\n",
    "        self.combine = combine  # Include combine in the environment\n",
    "        self.obstacles = obstacles\n",
    "        self.harvester_path = []\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.grid = np.full(self.grid_size, 1)  # Initialize entire grid with wheat\n",
    "        self.position_harvester = [0, 0]  # Example initial position\n",
    "        for pos in self.obstacles:\n",
    "            self.grid[pos[0], pos[1]] = -1  # Place obstacles\n",
    "        self.harvester_path = [list(self.position_harvester)]\n",
    "        return self._get_observation(), self._get_reward(), self._check_done(), {}\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Handle out of gasoline scenario\n",
    "        if self.combine.gasoline <= 0:\n",
    "            return self._get_observation(), RUN_OUT_GASOLINE, True, {}  # Use constant for penalty\n",
    "                                                \n",
    "        # Update the position of the harvester based on the action\n",
    "        if action == 0 and self.position_harvester[0] > 0:  # Up\n",
    "            self.position_harvester[0] -= 1 # X - 1\n",
    "        elif action == 1 and self.position_harvester[0] < self.grid_size[0] - 1:  # Down\n",
    "            self.position_harvester[0] += 1 # X + 1\n",
    "        elif action == 2 and self.position_harvester[1] > 0:  # Left\n",
    "            self.position_harvester[1] -= 1 # Y - 1\n",
    "        elif action == 3 and self.position_harvester[1] < self.grid_size[1] - 1:  # Right\n",
    "            self.position_harvester[1] += 1 # Y + 1\n",
    "\n",
    "        # Check for obstacles\n",
    "        if self.grid[self.position_harvester[0], self.position_harvester[1]] == -1:\n",
    "            return self._get_observation(), OBSTACLE_ENCOUNTERED, True, {}  # Use constant for penalty\n",
    "                        \n",
    "        if self.grid[self.position_harvester[0], self.position_harvester[1]] == 1:\n",
    "            if self.combine.wheat < self.combine.wheat_capacity:\n",
    "                self.combine.collect_wheat()  # Collect wheat\n",
    "                self.grid[self.position_harvester[0], self.position_harvester[1]] = 0  # Remove wheat from grid\n",
    "                                \n",
    "        observation = self._get_observation()\n",
    "        reward = self._get_reward()\n",
    "        done = self._check_done()\n",
    "        info = {}\n",
    "        return observation, reward, done, info    \n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        # Create a figure and axis\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(-0.5, self.grid_size[1] - 0.5)\n",
    "        ax.set_ylim(-0.5, self.grid_size[0] - 0.5)\n",
    "        plt.gca().invert_yaxis()\n",
    "        \n",
    "        # Plot wheat, obstacles, truck, and harvester\n",
    "        for i in range(self.grid_size[0]):\n",
    "            for j in range(self.grid_size[1]):\n",
    "                if [i, j] == self.position_harvester:\n",
    "                    ax.plot(j, i, 'bs', color='blue', label='Harvester')  # Harvester as blue square\n",
    "                elif self.grid[i, j] == 1:\n",
    "                    ax.plot(j, i, 'yo', label='Wheat')  # Wheat as yellow circle\n",
    "                elif self.grid[i, j] == -1:\n",
    "                    ax.plot(j, i, 'ko', label='Obstacle')  # Obstacle as black circle\n",
    "        \n",
    "        # Plot the path of the harvester\n",
    "        for pos in self.harvester_path:\n",
    "            ax.plot(pos[1], pos[0], 'b.')  # Path as blue dots\n",
    "\n",
    "        # Set the limits, labels, and title\n",
    "        ax.set_xlim(-0.5, self.grid_size[1] - 0.5)\n",
    "        ax.set_ylim(-0.5, self.grid_size[0] - 0.5)\n",
    "        ax.set_xlabel('X-axis')\n",
    "        ax.set_ylabel('Y-axis')\n",
    "        ax.set_title('Farm Environment')\n",
    "\n",
    "        # Show the plot\n",
    "        plt.gca().invert_yaxis()  # Invert y-axis to match the grid representation\n",
    "        plt.show()\n",
    "        \n",
    "    def _get_observation(self):\n",
    "        # Create a state representation\n",
    "        state = np.zeros(self.grid_size)\n",
    "\n",
    "        # Mark the position of the harvester\n",
    "        state[self.position_harvester[0], self.position_harvester[1]] = 2  # Representing the harvester\n",
    "\n",
    "        # Overlay the wheat and obstacles onto the state\n",
    "        state += self.grid\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def _get_reward(self):\n",
    "        # Example reward structure\n",
    "        reward = 0\n",
    "\n",
    "        # Positive reward for harvesting wheat\n",
    "        if self.grid[self.position_harvester[0], self.position_harvester[1]] == 1:\n",
    "            reward += WHEAT_HARVESTED  # Ensure this is a positive value           \n",
    "            # self.grid[self.position_harvester[0], self.position_harvester[1]] = 0  # Remove the wheat\n",
    "\n",
    "        # Negative reward for empty cell\n",
    "        if self.grid[self.position_harvester[0], self.position_harvester[1]] == 0:\n",
    "            reward += EMPTY_CELL\n",
    "\n",
    "        # Negative reward for gasoline spent\n",
    "        reward += GASOLINE_SPENT\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def _check_done(self):\n",
    "        # Check if all wheat has been harvested\n",
    "        if np.sum(self.grid == 1) == 0:\n",
    "            return True\n",
    "\n",
    "        # Other conditions, such as fuel depletion, can be added\n",
    "\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Combine:\n",
    "    def __init__(self, learning_rate=0.1, discount_factor=0.95, exploration_rate=1.0, gasoline_capacity=100, wheat_capacity=100):\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon = exploration_rate\n",
    "        self.q_table = {}\n",
    "        self.gasoline = gasoline_capacity  # Initial gasoline level\n",
    "        self.wheat = 0  # Initial wheat level\n",
    "        self.gasoline_capacity = gasoline_capacity\n",
    "        self.wheat_capacity = wheat_capacity\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        # Check if the agent has gasoline left\n",
    "        if self.gasoline <= 0:\n",
    "            return None  # or an appropriate action index if you have one for 'out of gasoline'\n",
    "        \n",
    "        state = tuple(state.flatten())\n",
    "\n",
    "        # Always choose a moving action\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: Randomly choose one of the moving actions\n",
    "            return np.random.choice([0, 1, 2, 3])  # Assumes these are moving actions\n",
    "        else:\n",
    "            # Exploit: Choose the best moving action based on the current Q-table\n",
    "            self.q_table.setdefault(state, np.zeros(5))\n",
    "            moving_actions_q_values = self.q_table[state][:4]  # Assumes the last action is not moving\n",
    "            return np.argmax(moving_actions_q_values)\n",
    "                \n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state = tuple(state.flatten())\n",
    "        next_state = tuple(next_state.flatten())\n",
    "        current_q = self.q_table.setdefault(state, np.zeros(5))[action]\n",
    "        max_future_q = np.max(self.q_table.setdefault(next_state, np.zeros(5)))\n",
    "        new_q = (1 - self.lr) * current_q + self.lr * (reward + self.gamma * max_future_q)\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def update_epsilon(self, min_epsilon=0.01, decay_rate=0.995):\n",
    "        if self.epsilon > min_epsilon:\n",
    "            self.epsilon *= decay_rate\n",
    "\n",
    "    def update_gasoline(self):\n",
    "        if self.gasoline > 0:\n",
    "            self.gasoline -= 1  # Consume gasoline for each action\n",
    "\n",
    "    def collect_wheat(self):\n",
    "        if self.wheat < self.wheat_capacity:\n",
    "            self.wheat += 1  # Collect wheat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(episodes, env, harvester):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()[0]  # Reset the environment and get initial state\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            action = harvester.choose_action(state)  # Choose an action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            harvester.learn(state, action, reward, next_state[0])  # Learn from the experience\n",
    "            state = next_state[0]\n",
    "            total_reward += reward\n",
    "\n",
    "        harvester.update_epsilon()  # Decrease exploration over time\n",
    "        print(f\"Episode: {episode}, Total Reward: {total_reward}\")\n",
    "        \n",
    "    # Return trained agents and environment for further use or analysis\n",
    "    # print(env.grid)\n",
    "    return harvester, env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Combine and FarmEnv\n",
    "combine = Combine()\n",
    "env = FarmEnv(grid_size=(10, 10), obstacles=[(1, 1), (3, 3)], combine=combine)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(100, env, combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def animation_plot(env, ax):\n",
    "    # Update plot for each frame\n",
    "    ax.clear()\n",
    "    grid = np.array(env.grid)  # Convert the grid to a NumPy array for visualization\n",
    "\n",
    "    # Display the field\n",
    "    ax.imshow(grid, cmap='YlGn_r', interpolation='none', alpha=0.8)\n",
    "\n",
    "    # Plot the position of the harvester\n",
    "    ax.scatter(env.position_harvester[1], env.position_harvester[0], color='blue', label='Harvester', s=100)\n",
    "\n",
    "    # Remove axis ticks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "def update(frame):\n",
    "    global combine, env, ax\n",
    "    if not hasattr(update, \"done\") or not update.done:\n",
    "        action = combine.choose_action(env._get_observation())\n",
    "        _, _, update.done, _ = env.step(action)\n",
    "        animation_plot(env, ax)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# ax.set_title(f\"Harvester gasoline: {combine.gasoline}\\nWheat: {combine.wheat}\")\n",
    "\n",
    "\n",
    "# Reset the environment for the animation\n",
    "env.reset()\n",
    "\n",
    "# Create the animation\n",
    "animation = FuncAnimation(fig, update, frames=range(100), interval=100)\n",
    "\n",
    "# Display the animation in Jupyter Notebook\n",
    "HTML(animation.to_jshtml())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
